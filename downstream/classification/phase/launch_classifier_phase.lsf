#!/bin/bash

#BSUB -nnodes 5
#BSUB -W 2:00
#BSUB -P stf218
#BSUB -alloc_flags "smt4 nvme"
#BSUB -J llm-classifier-phase-forge-s1-5
#BSUB -o logs/llm-classifier-phase-forge-s1-5.%J.o
#BSUB -e logs/llm-classifier-phase-forge-s1-5.%J.e
#BSUB -q debug

set +x

# load modules and conda
#module load open-ce/1.2.0-py38-0
#conda activate /gpfs/alpine/proj-shared/stf218/sajal/gitspace/LLM/downstream/ner/openceClone-new
module reset
module load open-ce/1.5.0-py38-0
conda activate $PWD/wip8p-new/

# export settings
export TORCH_EXTENSIONS_DIR=/gpfs/alpine/proj-shared/stf218/sajal/deepspeed
export HF_HOME=$PWD/../hfdata
export OMP_NUM_THREADS=1

export http_proxy=http://proxy.ccs.ornl.gov:3128/
export https_proxy=https://proxy.ccs.ornl.gov:3128/

# grab nodecount
nodes=($(cat ${LSB_DJOB_HOSTFILE} | sort | uniq | grep -v login | grep -v batch))
nnodes=${#nodes[@]}

# launch node config
rm -f `find -name *lock`    # clear stale lock files

jsrun --smpiargs="-disable_gpu_hooks" -n $nnodes -r 1 -g 6 -a 6 -c 42 python ./llm-classifier-phase-ds.py \
   --model_name_or_path "/gpfs/alpine/proj-shared/stf218/llm/model/forge-s1" \
   --output_dir ./outputs \
   --do_train=True \
   --per_device_train_batch_size 1 \
   --gradient_accumulation_steps 1 \
   --max_len 512 \
   --learning_rate 0.000002 \
   --adam_beta2 0.98 \
   --weight_decay 0.0000 \
   --adam_epsilon 1e-8 \
   --num_train_epochs 20 \
   --warmup_steps 20 \
   --logging_steps 50 \
   --logging_dir "run-logs" \
   --evaluation_strategy "epoch" \
   --logging_strategy "steps" \
   --deepspeed ds_config_zero.json
#
